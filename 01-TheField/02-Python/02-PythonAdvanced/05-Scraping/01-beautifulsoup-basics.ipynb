{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic data collection on the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start to tackle some nice web pages (HTML), we will discover the XML language which is a good introduction to scraping data on the web. XML = Extensible markup language 可扩展标记语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lists a few properties of the XML language.\n",
    "\n",
    "- XML was created to facilitate data exchange between machines and software.\n",
    "\n",
    "- XML is a language that is written using tags.\n",
    "\n",
    "- XML is a W3C recommendation, so it is a technology with strict rules to follow.\n",
    "\n",
    "- XML is intended to be understandable by everyone: people and machines alike.\n",
    "\n",
    "- XML allows us to create our own vocabulary using a set of customizable rules and tags.\n",
    "\n",
    "- XML is also compatible with the web so that data exchanges can be easily carried out over the Internet.\n",
    "\n",
    "- XML is therefore standardized, simple, but above all extensible and configurable so that any type of data can be described.\n",
    "下面列出了 XML 语言的一些特性。\n",
    "\n",
    "- 创建 XML 的目的是为了方便机器和软件之间的数据交换。\n",
    "\n",
    "- XML 是一种使用标记编写的语言。\n",
    "\n",
    "- XML 是 W3C 推荐的一种技术，因此有严格的规则可循。\n",
    "\n",
    "- XML 的目的是让所有人都能理解：无论是人还是机器。\n",
    "\n",
    "- XML 允许我们使用一套可定制的规则和标签创建自己的词汇表。\n",
    "\n",
    "- XML 还与网络兼容，因此可以很容易地在互联网上进行数据交换。\n",
    "\n",
    "- 因此，XML 是标准化的、简单的，但最重要的是可扩展和可配置的，因此可以描述任何类型的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a XML document, which we have saved as `data.xml` in the `assets/` directory.\n",
    "\n",
    "Display its content!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./assets/data.xml\"\n",
    "file = open(filename, \"r\")\n",
    "print(file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line indicates the encoding (we always stay in the UTF-8 encoding). Then we notice that the \"users\" tag has other \"user\" tags that themselves have their own tags. The data is hierarchized in a tree and each node provides information.第一行表示编码（我们始终使用 UTF-8 编码）。然后我们注意到，\"users\"（用户）标签有其他 \"user\"（用户）标签，这些标签本身也有自己的标签。数据以树形结构分级，每个节点都提供信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small script that displays all the usernames.这是一个显示所有用户名的小脚本。\n",
    "\n",
    "首先要安装 `lxml` 软件包。它依赖于 `numpy` 软件包，如果使用标准软件包管理器，它将与 `lxml` 一起安装。不过，某些版本的 `numpy` 会出现问题，因此，如果导入 `lxml` 失败，首先要解决的可能就是更改版本。\n",
    "\n",
    "You will first have to install the `lxml` package. It depends on the `numpy` package, which will be installed alongside `lxml` if you use a standard package manager. However, some version of `numpy` give problems, so changing the version might be the first thing that you can troubleshoot if you fail to import `lxml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lxml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlxml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m etree\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# I define my source document\u001b[39;00m\n\u001b[0;32m      4\u001b[0m tree \u001b[38;5;241m=\u001b[39m etree\u001b[38;5;241m.\u001b[39mparse(filename)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lxml'"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "\n",
    "# I define my source document\n",
    "tree = etree.parse(filename)\n",
    "# I look at my document and identify the path to the tag to get to the \"user\" information\n",
    "\n",
    "# The user name we are looking for is in it's own tag, `name`. Which itself\n",
    "# is in a tag `user`, and lastly `user` is contained in a `users` tag.\n",
    "# So tree.xpath(\"/users/user/name\") contains the tags associated with our search\n",
    "for user in tree.xpath(\"/users/user/name\"):\n",
    "    # I only want to display the content (.text) of the `/users/user/name` tags\n",
    "    print(user.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.xpath(\"/users/user/name\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can display the attributes of the tags that store this information\n",
    "tree = etree.parse(filename)\n",
    "for user in tree.xpath(\"/users/user\"):\n",
    "    print(user.get(\"data-id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can refine the display by proposing to display only users whose job is Veterinary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = etree.parse(filename)\n",
    "# Quel joli petit dictionnaire\n",
    "for user in tree.xpath(\"/users/user[job='Veterinary']/name\"):\n",
    "    print(user.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping\n",
    "\n",
    "Web scraping is a technique for automatically extracting information from websites. It is very useful in a lot of use cases:\n",
    "\n",
    "- **E-commerce**: create Excel sheet gathering all products from a provider's website\n",
    "- **Business prospecting**: extract contact information from websites (phone numbers, e-mail addresses...)\n",
    "- **Media analysis**: collect articles from online newspapers on a daily basis\n",
    "- ...\n",
    "\n",
    "Of course this task could be done by a human but this would be very painful and repetitive especially when the number of targeted pages is huge. To save time, we can automate it and, as usual, Python has a solution for that.\n",
    "\n",
    "The idea is to automate the human task by **fetching** pages and **extracting** data from them.\n",
    "\n",
    "**Fetching**\n",
    "\n",
    "Fetching is the downloading of the `html` source code of a page. It's exactly what your browser does when you open a website. To test it, open your browser, right-click anywhere on the page and select `view page source`. This is what Python will automate !\n",
    "\n",
    "**Extraction**\n",
    "\n",
    "From the `html` page you can extract, transform and load information in Python exactly like we did before with `xml`.\n",
    "\n",
    "Let's implement those two steps in few lines of code!\n",
    "eb scraping 是一种自动从网站中提取信息的技术。它在很多情况下都非常有用：\n",
    "\n",
    "- 电子商务**：创建 Excel 表，收集供应商网站上的所有产品信息\n",
    "- 业务拓展**：从网站上提取联系信息（电话号码、电子邮件地址......）。\n",
    "- 媒体分析**：每天从在线报纸上收集文章\n",
    "- ...\n",
    "\n",
    "当然，这项任务可以由人工完成，但这将是非常痛苦和重复的，尤其是当目标网页数量庞大时。为了节省时间，我们可以将其自动化，Python 也有相应的解决方案。\n",
    "\n",
    "我们的想法是通过**获取**页面并从中**提取**数据来自动完成人类的任务。\n",
    "\n",
    "**获取\n",
    "\n",
    "抓取是下载页面的源代码。这正是浏览器在打开网站时所要做的。要测试它，请打开浏览器，右键单击页面上的任意位置并选择 \"查看页面源代码\"。Python 将自动执行以下操作\n",
    "\n",
    "**提取**\n",
    "\n",
    "从 `html` 页面中，您可以用 Python 提取、转换和加载信息，就像我们之前用 `xml` 所做的那样。\n",
    "\n",
    "让我们用几行代码来实现这两个步骤！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching: Scraping via HTTP requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTTP (HyperText Transfer Protocol) is a protocol that will allow a **client** (you, through your browser or your code for example) to communicate with a **server** connected to the network (hosting a website or an internet document)\n",
    "\n",
    "Requests always go in pairs: the request (from the client) and the response (from the server).\n",
    "If this is not the case, it is because a problem has occurred at a point in the network.\n",
    "\n",
    "In Python we can use the library `requests` with the method `get`. It will emulate what you are doing manually in your browser.\n",
    "\n",
    "Start by installing the library using `pip install requests` or the `conda` package manager.\n",
    "\n",
    "Then let's take an URL, download the content and display it to see what happens.\n",
    "\n",
    "HTTP（超文本传输协议）是一种允许**客户端（例如你，通过浏览器或代码）与连接到网络上的**服务器（托管网站或互联网文档）进行通信的协议。\n",
    "\n",
    "请求总是成对的：请求（来自客户端）和响应（来自服务器）。\n",
    "如果不是这样，那是因为网络中的某个点出现了问题。\n",
    "\n",
    "在 Python 中，我们可以使用库 `requests` 和方法 `get`。它将模拟您在浏览器中手动执行的操作。\n",
    "\n",
    "首先使用 `pip install requests` 或 `conda` 软件包管理器安装该库。\n",
    "\n",
    "然后，让我们获取一个 URL，下载内容并显示它，看看会发生什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Url of website\n",
    "url = \"https://www.becode.org/about/\"\n",
    "# I send my HTTP request with a \"GET\" to the site server to identify in the url\n",
    "r = requests.get(url)\n",
    "# I display the requested url and the return of the server\n",
    "print(url, r.status_code)\n",
    "# I will store the content of the website in a variable and print it\n",
    "content = r.content\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the content is not easily readable. But like `xml`, `html` is a structured representation of the content. We then can use Python for reading it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction using Beautifulsoup\n",
    "\n",
    "For parsing `xml`we have used `xpath`. This is also possible for `html` ([see here](https://python-docs.readthedocs.io/en/latest/scenarios/scrape.html)).\n",
    "\n",
    "However there is a more _user-friendly_ library in Python that does the job quite well: `beautifulsoup`. We will use it in this notebook.\n",
    "\n",
    "First install it using `pip install beautifulsoup4` or the `conda` package manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the website content we just scraped in a Beautifulsoup object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(content, \"html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `html` tree of the page is now loaded and we can access its information. For example we can get the title of the page by recovering the content of the tag `h1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in soup.find_all(\"h1\"):\n",
    "    # We only retrieve the content ==> .text\n",
    "    print(tag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same with `h2` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, do the same with the `p` tags (that stand for _paragraphs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Choose an URL, get its content by using `requests`. Load it by using `beautifulsoup`. Then create a summary of the page by printing the content of `h1`, the first `h2` and the first paragraph (`p`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done, you did it ! Let's go with more advanced operation using `beautifulsoup`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('infosessionvenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "661c13da0699b4d3adfbe1192573631e3fbd9fa55405ad8c238e615a4e7e8a33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
